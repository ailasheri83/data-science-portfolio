 Silent Signals LLM Dog-Whistle Detection

A research codebase for evaluating Large Language Modelsâ€™ ability to detect and disambiguate coded political dogâ€whistles using the Silent Signals dataset and Metaâ€™s Llama 3.2 11B Vision Instruct Turbo.

---

ğŸ” Project Overview

Political dogâ€whistlesâ€”phrases with a hidden, audience-specific meaningâ€”are notoriously hard for conventional NLP systems to spot. This project:

1. **Detection**: Prompts Llama 3.2 11B Vision Turbo to decide whether each sentence contains a dogâ€whistle.  
2. **Disambiguation**: For each â€œrootâ€ whistle phrase, classifies individual sentences as coded (harmful) vs. innocent uses.  
3. **Embedding Analysis**: Compares the modelâ€™s â€œcoded meaningâ€ outputs against humanâ€curated definitions using cosine similarity, L1, and L2 distances and with the LLMs DeepSeek R1 Distilled Llama 70B, Llama 3.3 70B Instruct, 
Gemini 2.0 Flash.

All code and notebooks live here in `Silent-Signals-LLM-Dog-Whistle'.

ğŸ‘©â€ğŸ’» Aila Sheriâ€™s Contributions:

Prompt Engineering

- Designed the tight â€œJSON-onlyâ€ prompts that reliably elicit dog-whistle presence and coded-meaning outputs from Llama 3.2 11B Vision Turbo.

Pipeline Development

- Built the chunked detection loop to avoid rate-limits and parse JSON safely via regex.

- Implemented the per-root disambiguation routine, grouping sentences by â€œdog_whistle_rootâ€ and classifying coded vs. innocent usages.

Evaluation & Analysis

- Automated calculation of Accuracy, Precision, Recall, and F1 metrics for detection.

- Applied Sentence-Transformer embeddings to quantify semantic alignment (cosine, L1, L2) between model-generated meanings and human definitions.

Documentation & Reproducibility

- Authored this README and guided environment setup to ensure team members can reproduce experiments seamlessly.

ğŸ“ˆ Key Findings:

Detection: Llama 3.2 Vision Turbo achieved ~72 % accuracy (F1 â‰ˆ 0.73) on the Silent Signals detection task.

Disambiguation: Model reliably separates innocent vs. coded uses for many root phrases, though some sensitive terms required subâ€batching and lower temperature to avoid moderation refusals.

Semantic Alignment: Average cosine similarity between LLM meanings and human definitions was ~0.65, indicating goodâ€”but improvableâ€”alignment.


ğŸ¤ Acknowledgments: 

Dataset: SALT-NLP Silent Signals Detection Corpus

Model: Meta Llama 3.2 11B Vision Instruct Turbo via Together.ai

Team: Aila Sheri (LLM engineering & analysis for Llama 3.2 11B Vision Turbo), Alan Wu, Emma Carrier , Charlotte Zhao
